{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Curso: IA Generación de Prompts\n",
        "\n",
        "Comisión: 71380\n",
        "\n",
        "Estudiante: Terranova Nicolás Damián\n",
        "\n",
        "**Proyecto: Hands-on**\n",
        "\n",
        "\n",
        "PRESENTACIÓN DEL PROBLEMA\n",
        "\n",
        "Muchas personas pueden encontrarse en dificultades cuando un aparato electrónico o cualquier objeto sufre desperfectos o roturas. Por ejemplo: un lavarropas que comienza a hacer un ruido rechinante cuando gira el tambor, una lámpara de techo cuya luz no funciona, una heladera que pierde agua, el flotante de un tanque de agua que no funciona y no activa la bomba de agua haciendo que la casa quede sin agua, etc. La falta de conocimiento para reparar los artefactos o buscar el problema puede llevar a gastar dinero que no se tiene y se presentan situaciones en que no pueden hacerse los arreglos o no se cuenta con el dinero para solicitar una visita, ni se tiene conocimiento para revisar por sí mismos.\n",
        "Ello genera un impacto ya que se puede afectar el día a día de las personas por no contar con sus aparatos funcionando correctamente, pueden también verse a funcionamientos peligrosos por desperfectos o la imposibilidad de utilizarlos. Además, repararlos puede costar mucho dinero por requerir un especialista, la compra de repuestos, o la compra nuevamente del aparato.\n",
        "\n",
        "\n",
        "PROPUESTA DE SOLUCIÓN\n",
        "\n",
        "Utilización de IA, modelos de texto a texto y de texto a imagen, para generar un asistente virtual (que puede ser por empresa o marca) que proporcione conocimiento sobre la temática en particular que se desea resolver, procedimiento que sea necesario, aparatos, manuales, herramientas necesarias, medidas de seguridad y costos aproximados. También puede extenderse para hacer el seguimiento de una interacción, monitorear el progreso y ofrecer retroalimentación, como un apoyo a la reparación que necesita el usuario.\n",
        "La IA será provista de un rol, contexto y prompts de texto, el usuario detallará su problema y la API de openAI sugerirá lo que debe ser revisado y/o la manera de resolver el problema. Luego, DALLE-E podrá generar una imagen que sirva de referencia del aparato a revisar y sus componentes. A través de la técnica iterativa de prompting, se realizara una interacción como diálogo hasta que el usuario pueda despejar sus dudas.\n",
        "En un principio, las respuestas brindadas por el asistente serán generales (en base a las respuestas que puede generar actualmente la API de OpenAI). La manera en que el asistente podrá responder consultas específicas de aparatos por marca o aparatos antiguos es con entrenamiento, proveyendole manuales oficiales de los aparatos de cada marca, incorporando documentación oficial de services, con información provista por especialistas, acceso a foros (open book) e interacciones previas.\n",
        "Esa necesidad de entrenamiento, implica que esa respuestas específicas serán posibles de lograrse en el futuro.\n",
        "\n",
        "\n",
        "\n",
        "PROMPTS\n",
        "\n",
        "*Generación de respuesta a consulta (se detallará a modo de listado y en caso de tener pasos será una lista numerada)*\n",
        "\n",
        "Prompt: “Proporciona el detalle de pasos para reparar el siguiente aparato [nombre y modelo] cuando tiene el siguiente problema [problema]”\n",
        "\n",
        "Resultado Esperado: Listado detallado de lo que debe realizarse para reparar el inconveniente comentado por el usuario.\n",
        "\n",
        "Por ejemplo:\n",
        "Prompt: “Asistente, proporciona el detalle de pasos para reparar un lavarropas que realiza un ruido rechinante cada vez que el tambor interno gira\"\n",
        "\n",
        "Resultado Esperado:\n",
        "Un ruido rechinante en el tambor de un lavarropas generalmente puede deberse a varias causas. Aquí te menciono las más comunes:\n",
        "\n",
        "Rodamientos Dañados: Los rodamientos del tambor permiten que gire suavemente. Si están dañados o desgastados, pueden causar un ruido rechinante. El sonido puede ser más evidente cuando el tambor gira en ciertas velocidades.\n",
        "\n",
        "Polea o Correa Desgastada: La polea o la correa que ayuda a girar el tambor puede estar desgastada o dañada. Esto puede producir un ruido rechinante o chirriante.\n",
        "\n",
        "Para abordar el problema, aquí hay algunos pasos que puedes seguir:\n",
        "\n",
        "1.Desconecta el Lavarropas: Siempre es importante desconectar el aparato de la corriente eléctrica antes de realizar cualquier revisión.\n",
        "\n",
        "2.Inspecciona el Tambor: Revisa si hay objetos atrapados en el tambor o entre el tambor y el tambor interno. Puedes girar el tambor manualmente para verificar.\n",
        "\n",
        "Si después de realizar estas comprobaciones el problema persiste, o si no te sientes cómodo haciendo estas revisiones, te recomendaría contactar a un técnico especializado en reparación de electrodomésticos.\n",
        "\n",
        "\n",
        "*Generación de una imagen de las partes del aparato*\n",
        "\n",
        "Prompt: “Proporciona una imagen que muestre tanto interior como exterior de como se ve [parte del aparato].”\n",
        "\n",
        "Resultado Esperado: Imagen que muestre el detalle de como luce ese fragmento del aparato para comprender su funcionamiento.\n",
        "Además al ser Open Book la API podrá buscar imágenes en buscadores que reflejen la solicitud del usuario. La precisión mejorará con el entrenamiento. La imagen, en caso de ser generada, será generada por DALLE-E de acuerdo con el entrenamiento recibido.\n",
        "\n",
        "\n",
        "Por ejemplo:\n",
        "\n",
        "Prompt: “Asistente, proporciona una imagen que muestre dónde se encuentran los rodamientos del tambor de un lavarropas de carga frontal”\n",
        "\n",
        "Resultado esperado: https://www.google.com/url?sa=i&url=https%3A%2F%2Fm.youtube.com%2Fwatch%3Fv%3DKLV9X_SvWTw&psig=AOvVaw2561__77GToR5TW6fnEyhp&ust=1723239485055000&source=images&cd=vfe&opi=89978449&ved=0CBEQjRxqFwoTCLjRgvSu5ocDFQAAAAAdAAAAABAE\n",
        "\n",
        "*Retroalimentación*\n",
        "\n",
        "Prompt: “Sugiere qué debe revisarse en el siguiente aparato [nombre y modelo del aparato] cuando tiene el siguiente problema [problema] y se cumplen estas condiciones [condiciones] ”\n",
        "\n",
        "Resultado Esperado: Nuevo listado detallado con lo que debe realizarse para reparar el inconveniente comentado por el usuario.\n",
        "\n",
        "Por ejemplo:\n",
        "\n",
        "Prompt: “Asistente, proporciona una guía de posibles soluciones al rechinar del tambor de un lavarropas, cuando el lavarropas tiene carga superior en lugar de carga frontal”\n",
        "\n",
        "Resultado esperado: Para un lavarropas de carga superior, los problemas que causan ruidos rechinantes pueden ser similares a los de un lavarropas de carga frontal, pero la forma de acceso y diagnóstico puede variar. Aquí tienes una guía adaptada para un lavarropas de carga superior:\n",
        "\n",
        "Rodamientos Dañados: En los lavarropas de carga superior, los rodamientos suelen estar ubicados en la parte inferior del tambor. Si están desgastados o dañados, el tambor puede hacer un ruido rechinante al girar. Para inspeccionarlos, puede ser necesario levantar la máquina y retirar el tambor.\n",
        "\n",
        "Polea o Correa Desgastada: La polea y la correa en un lavarropas de carga superior se encuentran en la parte inferior del tambor. Un desgaste o daño en estos componentes puede causar ruidos. Inspecciona la correa para ver si está desgastada o rota y revisa la polea para verificar su estado.\n",
        "\n",
        "Pasos para Diagnosticar y Solucionar el Problema\n",
        "\n",
        "1.Desconecta el Lavarropas: Desconecta el aparato de la corriente eléctrica antes de comenzar cualquier inspección.\n",
        "\n",
        "2.Revisa el Tambor: Gira el tambor manualmente y escucha si hay algún ruido inusual. También, verifica si hay objetos atrapados en el tambor.\n",
        "\n",
        "3.Inspecciona la Correa y Polea: Accede a la parte inferior del tambor para inspeccionar la correa y la polea. Reemplaza cualquier componente desgastado o dañado.\n",
        "\n",
        "Si el problema persiste después de estas revisiones o si no te sientes cómodo realizando estos pasos, lo mejor es llamar a un técnico especializado. La reparación de componentes internos, como los rodamientos, puede ser compleja y puede requerir herramientas y experiencia especializada.\n",
        "\n",
        "\n",
        "JUSTIFICACIÓN DE VIABILIDAD:\n",
        "\n",
        "Recursos: La integración con una IA ya desarrollada como ChatGPT permite contar con una buena base de conocimientos que servirán de base a las respuestas. La limitación es el entrenamiento en los saberes específicos que la IA desconoce (por ejemplo, el manual de un modelo de lavarropas antiguo).\n",
        "La manera en que el asistente podrá responder consultas específicas es con entrenamiento, proveyendole manuales oficiales de los aparatos de cada marca, incorporando documentación oficial de services, acceso a foros (open book) e interacciones recurrentes con especialistas.\n",
        "Esa necesidad de entrenamiento, implica que esa respuestas específicas serán posibles de lograrse en el futuro.\n",
        "\n",
        "\n",
        "Accesibilidad: El proyecto sería accesible desde cualquier dispositivos con conexión a internet. Siendo que la mayor parte de la población cuenta al menos con un dispositivo móvil con conexión a internet. El proyecto no se propone abordar soluciones para la disponibilidad de internet ni de dispositivos. El proyecto mismo puede funcionar para mejorar la familiaridad de uso de los dispositivos (respondiendo a preguntas sobre, como cambiar la alarma de mi celular, cómo buscar en internet, etc., por ejemplo).\n",
        "\n",
        "Escalabilidad: La retroalimentación, así como el seguimiento de las soluciones brindadas a los usuarios permite que pueda crecer su capacidad de brindar soluciones a las distintas necesidades que se presenten. Además, los usuarios mismo pueden responder si el problema fue resuelto o no, ayudando a mejorar las respuestas.\n",
        "Las actualizaciones de la información se realizarán cada 6 meses con actualización de manuales e interacciones con especialistas en reparación y tecnología.\n"
      ],
      "metadata": {
        "id": "DtFQCSTTH7zu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Código"
      ],
      "metadata": {
        "id": "viU791b7IGmn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qha3XmGaHUjP",
        "outputId": "00fdf835-27e9-49de-e6b8-ecad74ed4598"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.5)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.10.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.7.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (2.3.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (24.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai==0.28"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key = \"sk-proj-ef0AReqBwW4Zn1J47tJsT3BlbkFJVApglGF18aKJej6v2Y63\""
      ],
      "metadata": {
        "id": "-0d7HKNMIhBE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Contexto\n",
        "context = 'Serás un asistente, con conocimientos de especialista técnico para la reparación de cualquier tipo de artefactos domésticos. Tu objetivo es ayudar a resolver problemas, sugerir qué se debe revisar y ayudar a la comprensión del funcionamiento del artefacto que mencionen los usuario. Además, generarás imagenes para que el usuario las utilice como referencia o buscaras el link para compartirlo en el caso de que exista en los resultados del buscador de goole'\n",
        "\n",
        "#Prompt (input del usuario)\n",
        "prompt = \"Asistente, proporciona el detalle de pasos para reparar un lavarropas que realiza un ruido rechinante cada vez que el tambor interno gira\""
      ],
      "metadata": {
        "id": "ARgghMDeI3V6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make the request\n",
        "conversation = [\n",
        "    {\"role\": \"system\", \"content\": context},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "\n",
        "response = openai.ChatCompletion.create(\n",
        "    model='gpt-3.5-turbo',\n",
        "    messages=conversation,\n",
        "    max_tokens=500,\n",
        ")\n",
        "\n",
        "message = response.choices[0]['message']\n",
        "print(\"{}: {}\".format(message['role'], message['content']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        },
        "id": "6sHgNF18JFXY",
        "outputId": "8fe73715-b6cd-4851-e288-f0a1da7185db",
        "collapsed": true
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-d4a897c87cfb>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m ]\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m response = openai.ChatCompletion.create(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gpt-3.5-turbo'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconversation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    766\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             )\n",
            "\u001b[0;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Storage the response in a variable to use it\n",
        "story_made = response['choices'][0]['message']['content'].strip()\n",
        "\n",
        "#Use the variable\n",
        "print(\"Respuesta:\", story_made)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "kkOqTOO_Jj3w",
        "outputId": "a6a18cf6-fdfb-4563-c2f1-740943df779b",
        "collapsed": true
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'response' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-95773522ac94>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Storage the response in a variable to use it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstory_made\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'choices'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#Use the variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Respuesta:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstory_made\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'response' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now use story_made as the context for a new prompt\n",
        "process_text_to_image = \"En base a la respuesta anterior crea un prompt que pueda generar una imagen que presente las condiciones estéticas de los planos que se ven en los manuales de usuario de los electrodomesticos\"\n",
        "# Add the new user message with story_made as the context\n",
        "conversation.append({\"role\": \"user\", \"content\": process_text_to_image})\n",
        "\n",
        "# Make the request for the new prompt\n",
        "response = openai.ChatCompletion.create(\n",
        "    model='gpt-3.5-turbo',\n",
        "    messages=conversation,\n",
        "    max_tokens=500\n",
        ")\n",
        "\n",
        "# Get and print the new story\n",
        "img_prompt = response['choices'][0]['message']['content'].strip()\n",
        "print(\"Prompt del artefacto:\", img_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "vyA3u627J-po",
        "outputId": "f4fb885a-41ef-46ea-8719-44e11ae4a197",
        "collapsed": true
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-6d231df8d587>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Make the request for the new prompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m response = openai.ChatCompletion.create(\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gpt-3.5-turbo'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconversation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    766\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             )\n",
            "\u001b[0;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now use story_made as the context for a new prompt\n",
        "image_from_scenario = f\"En base a la respuesta anterior, crea una imagen que represente el artefacto: {img_prompt}\"\n",
        "\n",
        "# Add the new user message with the image prompt\n",
        "conversation.append({\"role\": \"user\", \"content\": image_from_scenario})\n",
        "\n",
        "# Make the request for the image prompt\n",
        "image_response = openai.Image.create(\n",
        "    prompt=image_from_scenario,\n",
        "    n=1,\n",
        "    size=\"1024x1024\"\n",
        ")\n",
        "\n",
        "# Get and print the generated image URL\n",
        "print(image_response['data'][0]['url'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "s-TvswIYKTRf",
        "outputId": "ef242013-5fdb-48d8-f555-e9be6105f083"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'img_prompt' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-5938e5ad3b2a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Now use story_made as the context for a new prompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mimage_from_scenario\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"En base a la respuesta anterior, crea una imagen que represente el artefacto: {img_prompt}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Add the new user message with the image prompt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mconversation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimage_from_scenario\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'img_prompt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LfG0neM4LKzA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}